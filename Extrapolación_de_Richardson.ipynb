{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extrapolación de Richardson",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOr5L1SPG9i47nCjXYKXBqb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/janorena/modelado-y-simulacion/blob/master/Extrapolaci%C3%B3n_de_Richardson.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqhD6XKlUnQA"
      },
      "source": [
        "# Jorge Andrés Noreña García - 816543\n",
        "\n",
        "## Extrapolación de Richardson\n",
        "\n",
        "* método de Romberg."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNz8Gb8LVBV2"
      },
      "source": [
        "El método de extrapolación de Richardson, desarrollado por Lewis Fry Richardson (1881-1953), permite construir a partir de una secuencia convergente otra secuencia más rápidamente convergente. Esta técnica se usa frecuentemente para mejorar los resultados de métodos numéricos a partir de una estimación previa, de igual forma mejora la precisión en el cálculo numérico de la derivada de una función, partiendo de la base de la serie de Taylor. Este proceso es especialmente utilizado para definir un método de integración: el método de Romberg.\n",
        "\n",
        "### Principio\n",
        "\n",
        "Para una función variable en $\\mathrm{x},$ la primera derivada está definida por:\n",
        "\n",
        "$$\n",
        "f^{\\prime}(x)=\\lim _{h \\rightarrow 0} \\frac{f(x+h)-f(x)}{h}\n",
        "$$\n",
        "\n",
        "Una simple aproximación se tiene por la diferencia hacia adelante, de forma que:\n",
        "\n",
        "$$\n",
        "f_{1}(x)=\\frac{f(x+h)-f(x)}{h}\n",
        "$$\n",
        "\n",
        "Esta aproximación está lejos del valor real, por tanto para hacer un análisis del error, expandimos en forma de serie de Taylor:\n",
        "\n",
        "$$\n",
        "f(x+h)=f(x)+h f^{\\prime}(x)+\\frac{h^{2}}{2} f^{\\prime \\prime}(x)+\\frac{h^{3}}{3 !} f^{\\prime \\prime \\prime}(x)+\\cdots\n",
        "$$\n",
        "\n",
        "Substrayendo $f(x)$ de ambos lados y dividiendo por $h,$ se tiene que:\n",
        "\n",
        "$$\n",
        "f_{1}(x)=\\frac{f(x+h)-f(x)}{h}=f^{\\prime}(x)+\\frac{h}{2} f^{\\prime \\prime}(x)+\\frac{h^{2}}{3 !} f^{\\prime \\prime \\prime}(x)+\\cdots=f^{\\prime}(x)+O(h)\n",
        "$$\n",
        "\n",
        "Análogamente se derivan las demás fórmulas de aproximación, deduciendo por ejemplo, con diferencia hacia atrás 0 cambiando los valores de $\\mathrm{h} ;$ de esta forma se obtiene una expresión generalizada llamada extrapolación de Richardson:\n",
        "Sea $A,$ la respuesta exacta a la integral, $y A(h)$ la estimación de $A$ con orden $h^{k_{0}} .$ De tal forma que:\n",
        "\n",
        "$$\n",
        "A=\\lim _{h \\rightarrow 0} A(h)\n",
        "$$\n",
        "\n",
        "$$\n",
        "A=\\underbrace { A(h) }_{ O\\left( h^{ k_{ 0 } } \\right)  } +\\underbrace { a_{ 1 }h^{ k_{ 1 } }+\\underbrace { a_{ 2 }h^{ k_{ 2 } }+\\underbrace { a_{ 3 }h^{ k_{ 3 } }\\dashv \\cdots  }_{ O\\left( h^{ k_{ 3 } } \\right)  }  }_{ O\\left( h^{ k_{ 2 } } \\right)  }  }_{ O\\left( h^{ k_{ 1 } } \\right)  } \n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBtjGgb3VBSl"
      },
      "source": [
        "Donde:\n",
        "$O\\left(h^{k_{n}}\\right)$ es un estimador del error, usando la notación de Landau.\n",
        "$a_{1}, a_{2}, a_{3}, \\cdots \\quad y \\quad k_{1}, k_{2}, k_{3}, \\cdots$ son constantes desconocidas. Tal que $a_{i} \\neq 0 \\quad y \\quad k_{1}<k_{2}<k_{3}<\\cdots<k_{n}$ \n",
        "\n",
        "\n",
        "\n",
        "Ahora bien: Usando tamaños de espaciamiento $h$ y $h / t$, podemos aproximar a A como:\n",
        "\n",
        "\n",
        "$$\n",
        "A=A(h)+a_{1} h^{k_{1}}+O\\left(h^{k_{2}}\\right) \\quad(1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "A=A\\left(\\frac{h}{t}\\right)+a_{1}\\left(\\frac{h}{t}\\right)^{k_{1}}+O\\left(\\frac{h}{t}\\right)^{k_{2}}\n",
        "$$\n",
        "\n",
        "Multiplicando la última ecuación por $t^{k_{1}}$\n",
        "\n",
        "$$\n",
        "t^{k_{1}} A=t^{k_{1}} A\\left(\\frac{h}{t}\\right)+t^{k_{1}} a_{1}\\left(\\frac{h}{t}\\right)^{k_{1}}+t^{k_{1}} O\\left(\\frac{h}{t}\\right)^{k_{2}} \\quad(2)\n",
        "$$\n",
        "\n",
        "Sustrayendo (2) y (1), como se vio al inicio:\n",
        "\n",
        "$$\n",
        "\\left(t^{k_{1}}-1\\right) A=t^{k_{1}} A\\left(\\frac{h}{t}\\right)-A(h)+\\underbrace{t^{k_{1}} O\\left(\\frac{h}{t}\\right)^{k_{2}}-O\\left(h^{k_{2}}\\right)}_{O\\left(h_{h}^{k_{2}}\\right)}\n",
        "$$\n",
        "\n",
        "Despejando $\\mathrm{A}$ =\n",
        "\n",
        "$$\n",
        "A=\\frac{t^{k_{1}} A\\left(\\frac{h}{t}\\right)-A(h)}{\\left(t^{k_{1}}-1\\right)}+O\\left(h^{k_{2}}\\right)\n",
        "$$\n",
        "\n",
        "De este modo, se ha obtenido una mejor aproximación de A al sustraer el término más grande en el error, $O\\left(h^{k_{1}}\\right)$. De igual manera se pueden remover más términos de error de modo que se obtengan mejores aproximaciones de A. Una relación de recurrencia general puede ser implementada en las aproximaciones al hacer:\n",
        "\n",
        "$$\n",
        "A_{i+1}(h) \\approx \\frac{t^{k_{i}} A_{i}\\left(\\frac{h}{t}\\right)-A_{i}(h)}{\\left(t^{k_{i}}-1\\right)}, $$siendo $k_{i}$ el orden del error $$\n",
        "$$\n",
        "con:\n",
        "$$\n",
        "A=A_{i+1}(h)+O\\left(h^{k_{i}+1}\\right) \\quad y \\quad A_{1}=A(h)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8jvmobdVBOF"
      },
      "source": [
        "## Aplicaciones en métodos numéricos\n",
        "Las aplicaciones más inmediatas de la Extrapolación de Richardson en los métodos numéricos son dos: derivación numérica mediante diferencias centradas y utilizado para definir un método de integración de Romberg."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvjEImhnVBJl"
      },
      "source": [
        "En análisis numérico, el Método de Romberg genera una matriz triangular cuyos elementos son estimaciones numéricas de la integral definida siguiente:\n",
        "\n",
        "$$\n",
        "\\int_{a}^{b} f(x) d x\n",
        "$$\n",
        "\n",
        "usando la extrapolación de Richardson de forma reiterada en la regla del trapecio. El método de Romberg evalúa el integrando en puntos equiespaciados del intervalo de integración estudiado. Para que este método funcione, el integrando debe ser suficientemente derivable en el intervalo, aunque se obtienen resultados bastante buenos incluso para integrandos poco derivables. Aunque es posible evaluar el integrando en puntos no equiespaciados, en ese caso otros métodos como la cuadratura gaussiana o la cuadratura de Clenshaw–Curtis son más adecuados.\n",
        "\n",
        "## Metodo\n",
        "El método se define de forma recursiva asi:\n",
        "\n",
        "$$\n",
        "R(0,0)=\\frac{1}{2}(b-a)(f(a)+f(b))\n",
        "$$\n",
        "\n",
        "$$\n",
        "R(n,0)=\\frac { 1 }{ 2 } R(n-1,0)+h_{ n }\\sum _{ k=1 }^{ 2^{ n-1 } } f\\left( a+(2 k-1) h_{n} \\right) \n",
        "$$\n",
        "\n",
        "$$\n",
        "R(n, m)=R(n, m-1)+\\frac{1}{4^{m}-1}(R(n, m-1)-R(n-1, m-1))\n",
        "$$\n",
        "\n",
        "ó\n",
        "\n",
        "$$\n",
        "R(n, m)=\\frac{1}{4^{m}-1}\\left(4^{m} R(n, m-1)-R(n-1, m-1)\\right)\n",
        "$$\n",
        "\n",
        "donde:\n",
        "\n",
        "$$\n",
        "\\begin{array}{l}\n",
        "n \\geq 1 \\\\\n",
        "m \\geq 1 \\\\\n",
        "h_{n}=\\frac{b-a}{2^{n}}\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "La cota superior asintótica del error de $R(n, m)$ es:\n",
        "\n",
        "$$\n",
        "O\\left(h_{n}^{2^{m+1}}\\right)\n",
        "$$\n",
        "\n",
        "La extrapolación a orden cero $R(n, 0)$ es equivalente a la Regla del trapecio con $n+2$ puntos. a orden uno $R(n, 1)$ es equivalente a la Regla de Simpson con $n-2$ puntos.\n",
        "Cuando la evaluación del integrando es numéricamente costosa, es preferible reemplazar la interpolación polinómica de Richardson por la interpolación racional propuesta por Bulirsch y Stoer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1JJAtZFlu_i"
      },
      "source": [
        "Como ejemplo, se le integra la función gaussiana en el intervalo $[O, 1]$, esto es la función error evaluada en $1,$ cuyo valor es $\\operatorname{erf}(1) \\doteq 0.842700792949715$. Se calculan los elementos de la matriz triangular fila a fila, terminando los cálculos cuando la diferencia entre las dos últimas filas es menor que 1E-8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5fXKImpkWf7",
        "outputId": "3aead419-4437-409a-b238-06568fe1263a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from math import *\n",
        "\n",
        "def print_row(lst):\n",
        "    print(' '.join('%11.8f' % x for x in lst))\n",
        "\n",
        "def romberg(f, a, b, eps = 1E-8):\n",
        "    \"\"\"Approximate the definite integral of f from a to b by Romberg's method.\n",
        "    eps is the desired accuracy.\"\"\"\n",
        "    R = [[0.5 * (b - a) * (f(a) + f(b))]]  # R[0][0]\n",
        "    print_row(R[0])\n",
        "    n = 1\n",
        "    while True:\n",
        "        h = float(b-a)/2**n\n",
        "        R.append((n+1)*[None])  # Add an empty row.\n",
        "        R[n][0] = 0.5*R[n-1][0] + h*sum(f(a+(2*k-1)*h) for k in range(1, 2**(n-1)+1)) # for proper limits\n",
        "        for m in range(1, n+1):\n",
        "            R[n][m] = R[n][m-1] + (R[n][m-1] - R[n-1][m-1]) / (4**m - 1)\n",
        "        print_row(R[n])\n",
        "        if abs(R[n][n-1] - R[n][n]) < eps:\n",
        "            return R[n][n]\n",
        "        n += 1\n",
        "\n",
        "\n",
        "x=romberg(lambda t: 2/sqrt(pi)*exp(-t*t), 0, 1)\n",
        "# In this example, the error function erf(1) is evaluated.\n",
        "print(x)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 0.77174333\n",
            " 0.82526296  0.84310283\n",
            " 0.83836778  0.84273605  0.84271160\n",
            " 0.84161922  0.84270304  0.84270083  0.84270066\n",
            " 0.84243051  0.84270093  0.84270079  0.84270079  0.84270079\n",
            "0.8427007932686705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT2Yz5wZmskL"
      },
      "source": [
        "El resultado en la esquina inferior derecha de la matriz triangular es el resultado correcto con la precisión deseada. Nótese que este resultado se deriva de aproximaciones mucho peores obtenidas con la regla del trapecio mostradas aquí en la primera columna de la matriz triangular."
      ]
    }
  ]
}